{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce_slcsdcQnz"
      },
      "source": [
        "# Embeddings for Recommendation Systems\n",
        "\n",
        "As we’ve mentioned, the concept of embeddings is useful in so many other domains. In industry, it’s widely used for recommendation systems, for example.\n",
        "\n",
        "we’ll use the word2vec algorithm to embed songs using human-made music playlists. Imagine if we treated each song as we would a word or token, and we treated each playlist like a sentence. These embeddings can then be used to recommend similar songs that often appear together in playlists.\n",
        "\n",
        "The dataset we’ll use was collected by Shuo Chen from Cornell University. It contains playlists from hundreds of radio stations around the US. Figure 2-17 demonstrates this dataset.\n",
        "\n",
        "![Three playlists containing watched video IDs](../assets/videos_playlists.png)\n",
        "\n",
        "Figure 2-17. For video embeddings that capture video similarity we’ll use a dataset made up of a collection of playlists, each containing a list of videos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvGzcLOhdhYv"
      },
      "source": [
        "Let’s demonstrate the end product before we look at how it’s built. So let’s give it a few songs and see what it recommends in response.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QcGQCEAdky_"
      },
      "source": [
        "### Training a Song Embedding Model\n",
        "\n",
        "We’ll start by loading the dataset containing the song playlists as well as each song’s metadata, such as its title and artist:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HwJc8vMGcNBx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from urllib import request\n",
        "\n",
        "# Get the playlist dataset file\n",
        "#movies reccomendation system\n",
        "data_url = \"https://files.grouplens.org/datasets/movielens/ml-100k/u.data\"\n",
        "data = request.urlopen(data_url)\n",
        "\n",
        "lines = data.read().decode(\"utf-8\").split(\"\\n\")\n",
        "rows = [line.split(\"\\t\") for line in lines if len(line.split(\"\\t\")) == 4]\n",
        "df = pd.DataFrame(rows, columns=[\"user_id\", \"movie_id\", \"rating\", \"timestamp\"])\n",
        "\n",
        "df[\"user_id\"] = df[\"user_id\"].astype(int)\n",
        "df[\"movie_id\"] = df[\"movie_id\"].astype(int)\n",
        "\n",
        "# Load song metadata\n",
        "# songs_file = request.urlopen('https://storage.googleapis.com/maps-premium/dataset/yes_complete/song_hash.txt')\n",
        "# songs_file = songs_file.read().decode(\"utf-8\").split('\\n')\n",
        "# songs = [s.rstrip().split('\\t') for s in songs_file]\n",
        "# songs_df = pd.DataFrame(data=songs, columns = ['id', 'title', 'artist'])\n",
        "# songs_df = songs_df.set_index('id')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of rows:\", len(df))\n",
        "print(\"Number of unique users:\", df[\"user_id\"].nunique())\n",
        "print(\"Number of unique movies:\", df[\"movie_id\"].nunique())\n",
        "\n",
        "print(\"\\nSample of the dataset:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\nMost watched movies:\")\n",
        "print(df[\"movie_id\"].value_counts().head())\n",
        "\n",
        "print(\"\\nUsers most active:\")\n",
        "print(df[\"user_id\"].value_counts().head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEhEeA4Lu0Bo",
        "outputId": "bdcf4835-0555-402c-a039-d2345cee5477"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows: 100000\n",
            "Number of unique users: 943\n",
            "Number of unique movies: 1682\n",
            "\n",
            "Sample of the dataset:\n",
            "   user_id  movie_id rating  timestamp\n",
            "0      196       242      3  881250949\n",
            "1      186       302      3  891717742\n",
            "2       22       377      1  878887116\n",
            "3      244        51      2  880606923\n",
            "4      166       346      1  886397596\n",
            "\n",
            "Most watched movies:\n",
            "movie_id\n",
            "50     583\n",
            "258    509\n",
            "100    508\n",
            "181    507\n",
            "294    485\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Users most active:\n",
            "user_id\n",
            "405    737\n",
            "655    685\n",
            "13     636\n",
            "450    540\n",
            "276    518\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1S039eE-ctiR",
        "outputId": "6b51e573-26b2-4ec1-fb2f-95b3bbfd4f0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "movielist #1: ['61', '189', '33', '160', '20', '202', '171', '265', '155', '117', '47', '222', '253', '113', '227', '17', '90', '64', '92', '228', '266', '121', '114', '132', '74', '134', '98', '186', '221', '84', '31', '70', '60', '177', '27', '260', '145', '174', '159', '82', '56', '272', '80', '229', '140', '225', '235', '120', '125', '215', '6', '104', '49', '206', '76', '72', '185', '96', '213', '233', '258', '81', '78', '212', '143', '151', '51', '175', '107', '218', '209', '259', '108', '262', '12', '14', '97', '44', '53', '163', '210', '184', '157', '201', '150', '183', '248', '208', '128', '242', '148', '112', '193', '264', '219', '232', '236', '252', '200', '180', '250', '85', '91', '10', '254', '129', '241', '130', '255', '103', '118', '54', '267', '24', '86', '196', '39', '164', '230', '36', '23', '224', '73', '67', '65', '190', '100', '226', '243', '154', '214', '161', '62', '188', '102', '69', '170', '38', '9', '246', '22', '21', '179', '187', '135', '68', '146', '176', '166', '138', '247', '89', '2', '30', '63', '249', '269', '32', '141', '211', '40', '270', '133', '239', '194', '256', '220', '93', '8', '205', '234', '105', '147', '99', '1', '197', '173', '75', '268', '34', '144', '271', '119', '26', '158', '37', '181', '136', '257', '237', '131', '109', '182', '71', '223', '46', '169', '41', '162', '110', '66', '77', '199', '57', '50', '192', '178', '5', '87', '238', '156', '106', '167', '115', '11', '245', '35', '137', '127', '16', '79', '261', '45', '48', '25', '251', '195', '153', '101', '168', '123', '191', '4', '263', '203', '55', '42', '139', '240', '7', '149', '43', '165', '116', '198', '124', '95', '217', '58', '142', '216', '126', '83', '231', '204', '3', '207', '244', '19', '29', '18', '59', '15', '111', '52', '88', '13', '28', '172', '122', '152', '94']\n",
            "movielist #2: ['292', '251', '50', '314', '297', '290', '312', '281', '13', '280', '303', '308', '307', '257', '316', '315', '301', '313', '279', '299', '298', '19', '277', '282', '111', '258', '295', '242', '283', '276', '1', '305', '14', '287', '291', '293', '294', '310', '309', '306', '25', '273', '10', '311', '269', '255', '284', '274', '237', '300', '100', '127', '285', '289', '304', '272', '278', '288', '286', '275', '302', '296']\n"
          ]
        }
      ],
      "source": [
        "movielists = (\n",
        "    df.groupby(\"user_id\")[\"movie_id\"]\n",
        "    .apply(lambda x: list(map(str, x.tolist())))\n",
        "    .tolist()\n",
        ")\n",
        "print(\"\\nmovielist #1:\", movielists[0])\n",
        "print(\"movielist #2:\", movielists[1])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "meta_url = \"https://files.grouplens.org/datasets/movielens/ml-100k/u.item\"\n",
        "meta_raw = request.urlopen(meta_url).read().decode(\"latin-1\").split(\"\\n\")\n",
        "\n",
        "metadata = {}\n",
        "\n",
        "for row in meta_raw:\n",
        "    parts = row.split(\"|\")\n",
        "    if len(parts) > 2:\n",
        "        movie_id = parts[0]\n",
        "        title = parts[1]\n",
        "        metadata[movie_id] = title\n",
        "\n",
        "print(\"Movietitle:\", metadata[\"1\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2Rh7Ce-vIM2",
        "outputId": "b7766bc9-5a54-402c-9719-d77588547104"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Movietitle: Toy Story (1995)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iu4d9OBE0-Ug"
      },
      "source": [
        "Based on the official [Gensim Word2Vec documentation](https://radimrehurek.com/gensim/models/word2vec.html), here is the description for each parameter, of the next code snippet calling `Word2Vec`:\n",
        "\n",
        "* **`sentences` (playlists):** The input data. It must be an iterable of lists of tokens (in your case, song IDs or names within a playlist).\n",
        "* **`vector_size=32`:** The dimensionality of the word vectors. This defines the number of features in the hidden layer of the neural network used to represent each item.\n",
        "* **`window=20`:** The maximum distance between the current and predicted word within a sentence. A larger window captures more global context.\n",
        "* **`negative=50`:** Specifies how many \"noise words\" should be drawn for **Negative Sampling**. According to the documentation, values between 5 and 20 are typical for small datasets, while 2 to 5 suffice for large ones. You have set this high (50) to increase training rigor.\n",
        "* **`min_count=1`:** The model ignores all words with a total frequency lower than this. Setting it to 1 ensures every item in your playlists is included in the vocabulary.\n",
        "* **`workers=4`:** The number of worker threads used to train the model, allowing for multicore parallelization to speed up training."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQBx8je4221u",
        "outputId": "11df1787-f815-4b08-f030-96257eff2f60"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zQQMZdpbcwOK"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Train our Word2Vec model\n",
        "model = Word2Vec(\n",
        "    movielists,\n",
        "    vector_size=32,\n",
        "    window=20,\n",
        "    negative=50,\n",
        "    min_count=1,\n",
        "    workers=4\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8yH27BrcxfK",
        "outputId": "9f249988-242c-4ca0-e335-9dfc1c531767"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar movies to ID=50 which is star wars\n",
            "[('473', 0.9755904674530029), ('181', 0.9755661487579346), ('290', 0.9741015434265137), ('369', 0.9723175764083862), ('3', 0.9715413451194763), ('109', 0.9715346097946167), ('108', 0.9710854887962341), ('841', 0.970744252204895), ('1061', 0.9705591797828674), ('620', 0.9697421193122864)]\n"
          ]
        }
      ],
      "source": [
        "# song_id = 2172\n",
        "\n",
        "# # Ask the model for songs similar to song #2172\n",
        "# model.wv.most_similar(positive=str(song_id))\n",
        "\n",
        "movie_id = \"50\" # StarWars\n",
        "\n",
        "print(\"Most similar movies to ID=50 which is star wars\")\n",
        "print(model.wv.most_similar(positive=[movie_id]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcMPAXZ6c0wm",
        "outputId": "898397f9-d8ee-4bd7-c872-d06a8e71919e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Recommendations for movie 50:\n",
            "473  :  James and the Giant Peach (1996)\n",
            "181  :  Return of the Jedi (1983)\n",
            "290  :  Fierce Creatures (1997)\n",
            "369  :  Black Sheep (1996)\n",
            "3  :  Four Rooms (1995)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def print_movie_recommendations(movie_id):\n",
        "    similar_ids = np.array(\n",
        "        model.wv.most_similar(positive=[movie_id], topn=5)\n",
        "    )[:,0]\n",
        "\n",
        "    print(f\"\\nRecommendations for movie {movie_id}:\")\n",
        "    for mid in similar_ids:\n",
        "        print(mid, \" : \", metadata.get(mid, \"Unknown\"))\n",
        "\n",
        "print_movie_recommendations(\"50\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybkpP2xGdBGK",
        "outputId": "2442f033-ed92-487d-ae78-483b8c4356a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Recommendations for movie 100:\n",
            "9  :  Dead Man Walking (1995)\n",
            "475  :  Trainspotting (1996)\n",
            "276  :  Leaving Las Vegas (1995)\n",
            "150  :  Swingers (1996)\n",
            "13  :  Mighty Aphrodite (1995)\n"
          ]
        }
      ],
      "source": [
        "print_movie_recommendations(\"100\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAfQciFEdB15",
        "outputId": "33761a69-c092-4145-8ff9-7063488a56d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Recommendations for movie 181:\n",
            "596  :  Hunchback of Notre Dame, The (1996)\n",
            "274  :  Sabrina (1995)\n",
            "455  :  Jackie Chan's First Strike (1996)\n",
            "473  :  James and the Giant Peach (1996)\n",
            "756  :  Father of the Bride Part II (1995)\n"
          ]
        }
      ],
      "source": [
        "print_movie_recommendations(\"181\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}